### 1. ElasticSearch文档分值_score计算底层原理 

* 根据用户的query条件，先过滤出包含指定term的doc 
  * query "hello world" ‐‐> hello / world / hello & world 
  * bool ‐‐> must/must not/should ‐‐> 过滤 ‐‐> 包含 / 不包含 / 可能包含 
  * doc ‐‐> 不打分数 ‐‐> 正或反 true or false ‐‐> 为了减少后续要计算的doc的数量，提升性能 
* relevance score算法：简单来说，就是计算出，一个索引中的文本，与搜索文本， 他们之间的关联匹配程度。Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为 TF/IDF算法
  * Term frequency（词频）：搜索文本中的各个词条在field文本中出现了多少次，出现次数越 多，就越相关
  * Inverse document frequency（逆文档频率） ：搜索文本中的各个词条在整个索引的所有文档中出现 了多少次，出现的次数越多，就越不相关 

### 2. 分词器

* 默认的分词器（standard）：
  * standard tokenizer：以单词边界进行切分
  * standard token filter：什么都不做 
  * lowercase token filter：将所有字母转换为小写 
  * stop token filer（默认被禁用）：移除停用词，比如a the it等等 
* ik分词器
  * ik配置文件地址：es/plugins/ik/config目录 
  * IKAnalyzer.cfg.xml：用来配置自定义词库 
  * main.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起
  * quantifier.dic：放了一些单位相关的词 
  * suffix.dic：放了一些后缀 
  * surname.dic：中国的姓氏 
  * stopword.dic：英文停用词 
  * 自定义词库：
    * 自己补充自己的最新的词语，到ik的词库里面去。IKAnalyzer.cfg.xml：ext_dict，custom/mydict.dic 。补充自己的词语，然后需要重启es，才能生效 
    * 自己建立停用词库：比如了，的，啥，么，我们可能并不想去建立索引，让人家 搜索
  * IK热更新 
    * es不停机，直接我们在外部某个地方添加新的词语，es中立即热加载到这些新词语 IKAnalyzer.cfg.xml 



